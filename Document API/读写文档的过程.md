## 文档的读写
### 简介
Elasticsearch中的每个索引都分成多个shard ，每个shard可以有多个replica。这些replica被称为replication组，并且在添加或删除文档时必须保持同步。如果不这样做，就会导致数据的不一致。保持shard同步复制和提供读取操作的过程，我们称之为数据复制模型。  
Elasticsearch的数据复制模型基于主备份模型，在微软研究院的[PacificA论文](https://www.microsoft.com/en-us/research/publication/pacifica-replication-in-log-based-distributed-storage-systems/)中有很好的描述 。该模型基于具有充当主shard的复制组中的单个replica。其他replica称为replica shard。主要作为所有索引操作的主要入口点。它负责验证它们并确保它们是正确的。一旦索引操作被主服务器接受，主服务器也负责将操作复制到其他replica 。    
本节的目的是对Elasticsearch复制模型进行高度概述，并讨论它对写入和读取操作之间的各种交互的影响。  

### 基本写入模型  
Elasticsearch中的每个索引操作首先使用路由（通常基于文档ID）解析到复制组。一旦确定了复制组，操作就会在内部转发到组的当前主分片。主分片负责验证操作并将其转发给其他replica 。因为replica 离线（TODO），因此主节点不需要复制到所有replica 。相反，Elasticsearch维护应该接收操作的分片replica 列表。该列表称为同步replica 并由主节点维护，这个列表主要负责维护此不变量，因此必须将所有操作复制到此列表中的每个replica 。  

primary shard遵循以下基本流程：  
1. 验证传入参数的正确性，如果不正确，此时就拒绝这个操作。  
2. 在本地执行操作，即索引或者删除相关文档操作。这也将验证字段的内容，并在需要时拒绝（例如：关键字值在Lucene中索引太长）。  
3. 将操作转发到当前同步replica 集中的每个replica 。如果有多个replica ，这是并行完成的  
4. 一旦所有replica 都成功执行操作并响应主服务器，主服务器就会确认向客户端成功完成请求。

#### 写操作的容错处理  
索引期间很多事情都可能出错 - 磁盘可能损坏，节点可能互相断开连接，或者某些配置错误可能导致primary shard成功，但是replica 上的操作失败。这些并不常见，但主要还是要对它们做出回应。  

在主服务器本身发生故障的情况下会进行如下操作。  
1. 集群将会进行master选举 选举另外一个node为新的master。  
2. 新master将丢失掉的primary shard的某个replica shard提升为primary shard。此时cluster status会变为yellow。这是因为缺少了replica shard。  
3. 如果写发生了故障，那么给故障的节点发送请求，将副本同步删除（但是之前就保存了的数据不能删除）
4. 重启故障的node，new master将缺失的副本都copy一份到该node上去。而且该node会使用之前已有的shard数据，只是同步一下宕机之后发生的修改。  
cluster status变为green，因为都齐全了。  


#### 阅读模型   
Elasticsearch中的读取操作可以通过ID进行非常轻量级的查找，也可以通过复杂的聚合进行繁重的搜索请求。主备份模式的一个优点是任何一个分片都可以响应请求。  

当节点收到读取请求时，该节点负责将其转发到保存相关分片的节点，整理响应并响应客户端。我们称该节点为该请求的协调节点。基本流程如下：
1. 将读取请求解析为相关的shard。请注意，由于大多数搜索将被发送到一个或多个索引，它们通常需要从多个shard中读取，每个shard表示不同的数据子集。  
2. 对于每一个shard，可以从primary shard或者replica shard中任选一个。
3. 将shard级别的读取请求发送到选定的shard中。  
4. 将结果进行汇集。  

#### A few simple implications  
//TODO 待翻译    
Each of these basic flows determines how Elasticsearch behaves as a system for both reads and writes. Furthermore, since read and write requests can be executed concurrently, these two basic flows interact with each other. This has a few inherent implications:

**Efficient reads**  
Under normal operation each read operation is performed once for each relevant replication group. Only under failure conditions do multiple copies of the same shard execute the same search.  

**Read unacknowledged**  
Since the primary first indexes locally and then replicates the request, it is possible for a concurrent read to already see the change before it has been acknowledged.  

**Two copies by default**  
This model can be fault tolerant while maintaining only two copies of the data. This is in contrast to quorum-based system where the minimum number of copies for fault tolerance is 3.

#### 读操作的容错处理
如果读的过程中出现了文档，那么将换一个副本，读取另外的副本。有时候读操作会导致某些数据查不出来，但是也会有部分的响应，而不是说必须等待所有的问题都解决才有结果。  

#### 失败可能导致的问题  
在失败时，可能会导致一下问题：  
**减慢索引的速度**  
由于master每次操作期间都会等待所有同步副本的成功，因此单个慢分片会减慢整个复制组的速度。这是我们为上述读取效率付出的代价。  
**数据脏读**  
//TODO 还不是非常理解。  

**小提示**  
本文档提供了Elasticsearch如何处理数据的高级概述。当然，还有很多事情要做。诸如主要术语，集群状态发布和主选举等都在保持系统正常运行方面发挥了作用。本文档也不涵盖已知和重要的错误（包括关闭和打开），我们了解到[我们的项目github的issues](https://github.com/elastic/elasticsearch/issues?q=label%3Aresiliency)很难跟上这些错误。为了整理这些错误，帮助Elasticsearch纠错，我们有一个专门的[维护网站](https://www.elastic.co/guide/en/elasticsearch/resiliency/current/index.html) ,我们强烈建议阅读它。  


